#!/usr/bin/env python3
"""
√úr√ºn Adƒ± D√ºzeltme Sistemi - T√ºrk√ße Optimize
"""

import re
from typing import Dict, List, Tuple
from rapidfuzz import fuzz
import json

class TurkishProductNameCorrector:
    """T√ºrk√ße √ºr√ºn adƒ± d√ºzeltme sistemi"""
    
    def __init__(self):
        # T√ºrk√ße ekleri ve √ßekimleri
        self.turkish_suffixes = {
            # Gecelik √ßekimleri
            'geceliƒüi': 'gecelik', 'geceliƒüin': 'gecelik', 'geceliƒüe': 'gecelik',
            'gecelikler': 'gecelik', 'gecelig': 'gecelik', 'geceliƒüim': 'gecelik',
            
            # Pijama √ßekimleri
            'pijamayƒ±': 'pijama', 'pijamanƒ±n': 'pijama', 'pijamaya': 'pijama',
            'pijamalar': 'pijama', 'pijamalarƒ±': 'pijama', 'pijamam': 'pijama',
            
            # Elbise √ßekimleri
            'elbiseyi': 'elbise', 'elbisenin': 'elbise', 'elbiseye': 'elbise',
            'elbiseler': 'elbise', 'elbiseleri': 'elbise', 'elbisem': 'elbise',
            
            # Sabahlƒ±k √ßekimleri
            'sabahlƒ±ƒüƒ±': 'sabahlƒ±k', 'sabahlƒ±ƒüƒ±n': 'sabahlƒ±k', 'sabahlƒ±ƒüa': 'sabahlƒ±k',
            'sabahlƒ±klar': 'sabahlƒ±k', 'sabahlƒ±klarƒ±': 'sabahlƒ±k',
            
            # Takƒ±m √ßekimleri
            'takƒ±mƒ±': 'takƒ±m', 'takƒ±mƒ±n': 'takƒ±m', 'takƒ±ma': 'takƒ±m',
            'takƒ±mlar': 'takƒ±m', 'takƒ±mlarƒ±': 'takƒ±m', 'takimi': 'takƒ±m'
        }
        
        # Yaygƒ±n yazƒ±m hatalarƒ±
        self.typo_corrections = {
            'gecelig': 'gecelik',
            'pijama': 'pijama',  # Doƒüru yazƒ±m
            'pyjama': 'pijama',
            'pajama': 'pijama',
            'afirca': 'afrika',
            'afirka': 'afrika',
            'hamile': 'hamile',  # Doƒüru yazƒ±m
            'lohusa': 'lohusa',  # Doƒüru yazƒ±m
            'danteli': 'dantelli',
            'danteli': 'dantelli'
        }
        
        # Renk normalizasyonu
        self.color_mapping = {
            'siyah': ['siyah', 'black', 'kara'],
            'beyaz': ['beyaz', 'white', 'ak'],
            'kƒ±rmƒ±zƒ±': ['kƒ±rmƒ±zƒ±', 'red', 'al'],
            'mavi': ['mavi', 'blue', 'g√∂k'],
            'ye≈üil': ['ye≈üil', 'green'],
            'sarƒ±': ['sarƒ±', 'yellow'],
            'pembe': ['pembe', 'pink', 'roz√©'],
            'mor': ['mor', 'purple', 'menek≈üe'],
            'lacivert': ['lacivert', 'navy', 'koyu mavi'],
            'bordo': ['bordo', 'burgundy', 'koyu kƒ±rmƒ±zƒ±'],
            'bej': ['bej', 'beige', 'krem'],
            'gri': ['gri', 'gray', 'grey', 'k√ºl'],
            'ekru': ['ekru', 'cream', 'krem']
        }
        
        # √ñzel √ºr√ºn kombinasyonlarƒ±
        self.special_combinations = {
            ('afrika', 'gecelik'): 'afrika gecelik',
            ('africa', 'gecelik'): 'afrika gecelik',
            ('hamile', 'gecelik'): 'hamile gecelik',
            ('hamile', 'pijama'): 'hamile pijama',
            ('lohusa', 'gecelik'): 'lohusa gecelik',
            ('pijama', 'takƒ±m'): 'pijama takƒ±mƒ±',
            ('pijama', 'takimi'): 'pijama takƒ±mƒ±'
        }
    
    def correct_product_name(self, query: str) -> str:
        """Ana d√ºzeltme fonksiyonu"""
        if not query:
            return ""
        
        # 1. Temel temizlik
        corrected = self._basic_cleanup(query)
        
        # 2. Yazƒ±m hatasƒ± d√ºzeltme
        corrected = self._fix_typos(corrected)
        
        # 3. T√ºrk√ße ek d√ºzeltme
        corrected = self._fix_turkish_suffixes(corrected)
        
        # 4. Renk normalizasyonu
        corrected = self._normalize_colors(corrected)
        
        # 5. √ñzel kombinasyonlar
        corrected = self._handle_special_combinations(corrected)
        
        # 6. Son temizlik
        corrected = self._final_cleanup(corrected)
        
        return corrected
    
    def _basic_cleanup(self, text: str) -> str:
        """Temel temizlik"""
        # K√º√ß√ºk harfe √ßevir
        text = text.lower().strip()
        
        # Fazla bo≈üluklarƒ± temizle
        text = re.sub(r'\s+', ' ', text)
        
        # √ñzel karakterleri temizle (ama T√ºrk√ße karakterleri koru)
        text = re.sub(r'[^\w\s√ßƒüƒ±√∂≈ü√º√áƒûIƒ∞√ñ≈û√ú]', ' ', text)
        
        return text.strip()
    
    def _fix_typos(self, text: str) -> str:
        """Yazƒ±m hatalarƒ±nƒ± d√ºzelt"""
        words = text.split()
        corrected_words = []
        
        for word in words:
            # Direkt e≈üle≈üme
            if word in self.typo_corrections:
                corrected_words.append(self.typo_corrections[word])
            else:
                # Fuzzy matching ile yakƒ±n kelime bul
                best_match = None
                best_score = 0
                
                for correct_word in self.typo_corrections.values():
                    score = fuzz.ratio(word, correct_word)
                    if score > 80 and score > best_score:
                        best_score = score
                        best_match = correct_word
                
                corrected_words.append(best_match or word)
        
        return ' '.join(corrected_words)
    
    def _fix_turkish_suffixes(self, text: str) -> str:
        """T√ºrk√ße ekleri d√ºzelt"""
        words = text.split()
        corrected_words = []
        
        for word in words:
            # Direkt e≈üle≈üme
            if word in self.turkish_suffixes:
                corrected_words.append(self.turkish_suffixes[word])
            else:
                # Kƒ±smi e≈üle≈üme (kelime i√ßinde ek var mƒ±?)
                corrected = word
                for suffix, root in self.turkish_suffixes.items():
                    if suffix in word and word != suffix:
                        # Kelimenin ba≈üƒ±nda root var mƒ± kontrol et
                        if word.startswith(root) or root in word:
                            corrected = root
                            break
                
                corrected_words.append(corrected)
        
        return ' '.join(corrected_words)
    
    def _normalize_colors(self, text: str) -> str:
        """Renkleri normalize et"""
        words = text.split()
        normalized_words = []
        
        for word in words:
            color_found = False
            
            # Her renk i√ßin kontrol et
            for standard_color, variations in self.color_mapping.items():
                if word in variations:
                    normalized_words.append(standard_color)
                    color_found = True
                    break
            
            if not color_found:
                normalized_words.append(word)
        
        return ' '.join(normalized_words)
    
    def _handle_special_combinations(self, text: str) -> str:
        """√ñzel kombinasyonlarƒ± handle et"""
        words = text.split()
        
        # ƒ∞ki kelimeli kombinasyonlarƒ± kontrol et
        for i in range(len(words) - 1):
            combination = (words[i], words[i + 1])
            if combination in self.special_combinations:
                # Kombinasyonu deƒüi≈ütir
                result = self.special_combinations[combination]
                # Yeni listeyi olu≈ütur
                new_words = words[:i] + result.split() + words[i + 2:]
                return ' '.join(new_words)
        
        # √ú√ß kelimeli kombinasyonlarƒ± da kontrol edebiliriz
        for i in range(len(words) - 2):
            # √ñrnek: "afrika style gecelik" ‚Üí "afrika gecelik"
            if words[i] == 'afrika' and words[i + 2] == 'gecelik':
                new_words = words[:i] + ['afrika', 'gecelik'] + words[i + 3:]
                return ' '.join(new_words)
        
        return text
    
    def _final_cleanup(self, text: str) -> str:
        """Son temizlik"""
        # Fazla bo≈üluklarƒ± temizle
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Bo≈ü string kontrol√º
        if not text:
            return ""
        
        return text
    
    def get_correction_confidence(self, original: str, corrected: str) -> float:
        """D√ºzeltme g√ºven skoru"""
        if original == corrected:
            return 1.0
        
        # Fuzzy similarity
        similarity = fuzz.ratio(original.lower(), corrected.lower()) / 100
        
        # Uzunluk farkƒ± penaltƒ±sƒ±
        length_diff = abs(len(original) - len(corrected)) / max(len(original), len(corrected))
        length_penalty = max(0, 1 - length_diff)
        
        return similarity * length_penalty

def test_correction_system():
    """D√ºzeltme sistemini test et"""
    corrector = TurkishProductNameCorrector()
    
    test_cases = [
        "afrika geceliƒüi",
        "afirca gecelig",
        "hamile pijamalarƒ±",
        "siyah gecelik",
        "pijama takimi",
        "danteli gecelik",
        "lohusa sabahlƒ±ƒüƒ±",
        "AFRIKA GECELƒ∞K",
        "beyaz pyjama",
        "kƒ±rmƒ±zƒ± elbiseyi"
    ]
    
    print("üîß √úr√ºn Adƒ± D√ºzeltme Testi")
    print("=" * 50)
    
    for test_case in test_cases:
        corrected = corrector.correct_product_name(test_case)
        confidence = corrector.get_correction_confidence(test_case, corrected)
        
        print(f"'{test_case}' ‚Üí '{corrected}' (G√ºven: {confidence:.2f})")

def integrate_with_current_system():
    """Mevcut sisteme entegrasyon √∂rneƒüi"""
    print("\nüîó Mevcut Sisteme Entegrasyon")
    print("=" * 40)
    
    print("LLM Service'e eklenecek kod:")
    print("""
def _extract_product_name(self, message: str) -> str:
    # √ñnce d√ºzeltme sistemi
    corrector = TurkishProductNameCorrector()
    corrected_message = corrector.correct_product_name(message)
    
    # Sonra mevcut extraction logic
    return self._extract_from_corrected_message(corrected_message)
""")
    
    print("\nDatabase Service'e eklenecek kod:")
    print("""
async def get_product_info(self, business_id: str, product_name: str):
    # √ñnce d√ºzeltme
    corrector = TurkishProductNameCorrector()
    corrected_name = corrector.correct_product_name(product_name)
    
    # Sonra mevcut search logic
    return await self._search_with_corrected_name(corrected_name)
""")

if __name__ == "__main__":
    test_correction_system()
    integrate_with_current_system()